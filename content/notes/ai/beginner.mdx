---
title: "AI Foundations"
description: "A friendly but serious introduction to data, models and how AI systems work."
level: "foundations"
courseId: "ai"
levelId: "foundations"
summary: "A friendly but serious introduction to data, models and how AI systems work, written as my own notes."
estimatedHours: 8
stepIndex: 0
learningObjectives:
  - "Understand core AI vocabulary (features, labels, training, validation, testing) well enough to explain it clearly."
  - "Explain how simple models learn patterns from data and where they commonly fail."
  - "Apply basic checks for leakage, overfitting, and threshold trade offs using the labs."
  - "Evaluate model outputs with a beginner friendly view of quality, bias, and risk."
---

import ToolCard from "@/components/notes/ToolCard"
import Callout from "@/components/notes/Callout"
import GlossaryTip from "@/components/notes/GlossaryTip"
import QuizBlock from "@/components/notes/QuizBlock"
import PageNav from "@/components/notes/PageNav"
import SectionProgressToggle from "@/components/notes/SectionProgressToggle"
import LevelProgressBar from "@/components/course/LevelProgressBar"
import CPDTracker from "@/components/CPDTracker"
import DiagramBlock from "@/components/DiagramBlock"
import SectionHeader from "@/components/course/SectionHeader"
import SubsectionHeader from "@/components/course/SubsectionHeader"
import BodyText from "@/components/course/BodyText"
import { aiSectionManifest } from "@/lib/aiSections"
import ClusteringIntuitionTool from "@/components/notes/tools/ai/beginner/ClusteringIntuitionTool"
import ResponsibleAIPlannerTool from "@/components/notes/tools/ai/beginner/ResponsibleAIPlannerTool"
import AIExamplesExplorerTool from "@/components/notes/tools/ai/beginner/AIExamplesExplorerTool"
import VectorVisualiserTool from "@/components/notes/tools/ai/beginner/VectorVisualiserTool"
import TinyClassifierLabTool from "@/components/notes/tools/ai/beginner/TinyClassifierLabTool"

# AI Foundations

<LevelProgressBar courseId="ai" levelId="foundations" sectionIds={aiSectionManifest.foundations} />

<CPDTracker courseId="ai" levelId="foundations" estimatedHours={8} />

<BodyText>
  These notes are a calm path into AI. I focus on meaning first, numbers second, and practice always. The goal is not buzzwords. The goal is to build judgement you can use when you meet real systems.
</BodyText>

---

## What AI is and why it matters now

<SectionHeader variant="content" emoji="üß†" id="what-is-ai">
  What AI is and why it matters now
</SectionHeader>

<SectionProgressToggle courseId="ai" levelId="foundations" sectionId="ai-foundations-what-is-ai" />

<Callout variant="concept">
AI is a way of learning patterns from data so a system can make predictions, rank options, or automate decisions.
</Callout>

When people say AI, they often mean a system that takes input, applies a learned pattern, and produces an output. The learned pattern is the <GlossaryTip term="model">A function learned from data that maps inputs to outputs.</GlossaryTip>. The act of learning that pattern is <GlossaryTip term="training">The process of fitting a model to data so it can learn patterns.</GlossaryTip>. Using the trained model to produce results is <GlossaryTip term="inference">Running a trained model to produce predictions on new inputs.</GlossaryTip>.

The difference between training and inference matters because the risks are different. Training is where you bake in assumptions from the <GlossaryTip term="dataset">A collection of examples used to train and evaluate a model.</GlossaryTip>. Inference is where the model meets reality. If reality changes, the model can behave badly even if training looked perfect.

AI is powerful because it can learn patterns too complex to write as hand made rules. It is also fragile because it can learn the wrong pattern. A model can look clever while failing quietly. The skill is to ask what it is really using as evidence.

AI matters now because systems touch decisions that used to be manual. Hiring screens, fraud checks, support routing, and medical triage all use models to move faster. That speed is useful, but it can also amplify mistakes at scale. This is why foundations matter. I need to know what the model is doing before I trust it.

Imagine a support team that uses an AI model to route urgent messages. If the model learns that certain phrases usually mean ‚Äúurgent‚Äù, it may quietly miss urgent messages written in a different style. The risk is not only wrong answers. The risk is wrong priorities at speed.

In practice, the first useful question is ‚ÄúWhat happens on the model‚Äôs bad day?‚Äù If the answer is ‚Äúwe do not know‚Äù, the system is not ready to be trusted for anything high impact.

My calm view on hype is this. AI is not magic. It is applied statistics plus engineering. It will keep changing how work is done, and it will keep producing failures that look silly in hindsight. If you learn the basics, you can use it well without believing the marketing.

<DiagramBlock
  title="Rules based software vs model based systems"
  subtitle="Rules are explicit. Models are learned from data."
>
  <div className="text-xs sm:text-sm leading-5 space-y-2">
    <div>Rules: if condition then action</div>
    <div>Model: input -> model -> output</div>
    <div>Training builds the model. Inference uses it.</div>
    <div>Rules are easier to explain. Models need monitoring and care.</div>
  </div>
</DiagramBlock>

<ToolCard
  id="ai-examples-explorer"
  title="Explore simple AI examples"
  description="Toggle between examples like spam filters, recommendation systems and chat assistants, and see what each one takes as input and gives as output."
>
  <AIExamplesExplorerTool />
</ToolCard>

<QuizBlock
  id="ai-foundations-what-is-ai-quiz"
  courseId="ai"
  levelId="foundations"
  sectionId="ai-foundations-what-is-ai"
  title="Quick check. What is AI"
  questions={[
    { q: "What is a model in AI", a: "A learned function that maps inputs to outputs." },
    { q: "What is training", a: "Fitting a model to data so it learns patterns." },
    { q: "What is inference", a: "Using a trained model to make predictions on new inputs." },
    { q: "Give one everyday AI example", a: "Spam filtering, recommendations, or fraud detection." },
    { q: "Why does AI matter now", a: "It scales decisions that used to be manual and can amplify impact." },
    { q: "What is one risk of AI at scale", a: "Mistakes spread faster and affect more people." },
    { q: "What does AI output usually look like", a: "A label, score, or ranked list." },
    { q: "What is generalisation", a: "When a model performs well on new data, not just the training data." },
    { q: "Why do models need monitoring after launch", a: "Because real inputs change and models can drift or fail silently." },
    { q: "What is one reason a rule based system may be preferred", a: "It is easier to explain and can be safer for simple, high assurance requirements." },
    { q: "What does it mean if a model uses a shortcut feature", a: "It learned a proxy that works in training but may fail in real use." },
    { q: "What is a practical habit when reading AI claims", a: "Ask what data it learned from, what it outputs, and what happens when it is wrong." },
  ]}
/>

<SubsectionHeader emoji="‚úÖ">
  After this section you should be able to:
</SubsectionHeader>

1. Explain what a model is and why it exists in a system
2. Explain what breaks when training data and real inputs diverge
3. Explain the trade off between automation speed and human judgement

---

## Data and representation

<SectionHeader variant="content" emoji="üìä" id="data-and-representation">
  Data and representation
</SectionHeader>

<SectionProgressToggle courseId="ai" levelId="foundations" sectionId="ai-foundations-data-and-representation" />

In AI, the word <GlossaryTip term="data">
Data is recorded observations about the world. It is what the model learns from, not what we wish was true.
</GlossaryTip> sounds fancy, but it is usually boring. It is clicks, purchases, support tickets, photos, sensor readings, and text. Data always comes with context. Where did it come from. Who produced it. What is missing. What was measured badly. If you ignore that, you build a confident model on shaky ground.

Some data is structured. That means it fits neatly into rows and columns. Think customer age, number of failed logins, or time since last password reset. Other data is unstructured. That means it looks like raw text, images, audio, or long logs. It still has structure, but you have to extract it.

To train a model, we usually separate inputs from the answer we want. A <GlossaryTip term="feature">
A feature is a measurable piece of information about something. For example the length of an email or the number of failed login attempts.
</GlossaryTip> is an input signal. A <GlossaryTip term="label">
A label is the correct answer for training. For example spam or not spam, or refund needed or not.
</GlossaryTip> is the outcome we want the model to learn to predict.

Models cannot understand raw text or images the way humans do. They do not see meaning. They see numbers. If you give a model a photo, it will be turned into numbers first. If you give it an email, it will be turned into numbers first. The model learns patterns in those numbers.

The simplest numeric form is a <GlossaryTip term="vector">
A vector is a list of numbers that represents an input. The numbers are chosen so the model can work with them.
</GlossaryTip>. For text, we first break it into a <GlossaryTip term="token">
A token is a small chunk of text used as the unit for language models. It might be a word or part of a word.
</GlossaryTip>. Then we map those tokens into an <GlossaryTip term="embedding">
An embedding is a numeric vector that tries to place similar items near each other in number space.
</GlossaryTip>.

The intuition is simple. If two pieces of text are used in similar ways, they often end up with similar numbers. A model can then treat closeness as a hint that the meaning is related. It is not perfect. It is a useful shortcut.

In a real system, representation choices show up as behaviour. If you represent a customer only by ‚Äúspend last month‚Äù, the model may miss that a loyal customer is having a temporary issue. If you represent them by richer behaviour signals, the model may be more useful but also harder to explain.

Suppose you build a model using ‚Äúpostcode‚Äù as a feature because it predicts outcomes well. In practice, that can become a proxy for protected attributes. The representation can silently encode social patterns you did not intend to automate.

Bad data creates bad models. If the labels are wrong, the model learns the wrong lesson. If the data is missing whole groups of people, the model will fail on those groups. If the data reflects old behaviour, the model will struggle when the world changes. This is why data work is not busywork. It is the foundation.

Splitting data matters because we want honest feedback. Training data is what the model learns from. Validation data is what you use to make choices during building. Test data is the final check you keep separate until the end. If you test on the same data you trained on, you are grading your own homework with the answer sheet open.

<DiagramBlock title="From raw data to numbers a model can learn from" subtitle="Turn messy inputs into numeric representation.">
  <div className="text-xs sm:text-sm leading-5 space-y-2">
    <div>Raw text or images</div>
    <div>Cleaning and preparation</div>
    <div>Features chosen from the data</div>
    <div>Numeric representation as vectors and embeddings</div>
    <div>Model input</div>
  </div>
</DiagramBlock>

<ToolCard
  id="vector-visualiser"
  title="See how data becomes numbers"
  description="Type in a few short phrases and see how they turn into numeric vectors, then compare how similar they are."
>
  <VectorVisualiserTool />
</ToolCard>

<QuizBlock
  id="ai-foundations-data-quiz"
  courseId="ai"
  levelId="foundations"
  sectionId="ai-foundations-data-and-representation"
  title="Quick check. Data and representation"
  questions={[
    { q: "In AI, what does data mean", a: "Recorded observations about the world that the model learns from." },
    { q: "What is the difference between structured and unstructured data", a: "Structured fits rows and columns, unstructured is text, images, audio, or logs that need processing." },
    { q: "What is a feature", a: "A measurable input signal used by the model." },
    { q: "What is a label", a: "The correct answer used for training." },
    { q: "Why can models not understand raw text or images directly", a: "They operate on numbers, so inputs must be turned into numeric form." },
    { q: "What is a vector in this context", a: "A list of numbers that represents an input." },
    { q: "What is an embedding for", a: "To represent items as numbers so similar items end up near each other." },
    { q: "Why do similar things often end up with similar numbers", a: "The representation is trained to capture patterns of use and meaning as closeness." },
    { q: "Why do we split data into training, validation, and test sets", a: "To learn, tune choices, and then do an honest final check without cheating." },
    { q: "Name one way bad data creates bad models", a: "Wrong labels, missing groups, or outdated data lead to confident but wrong behaviour." },
    { q: "What is data leakage in simple terms", a: "When information that should not be available sneaks into training or evaluation, making results look better than reality." },
    { q: "Why is a single metric rarely enough", a: "Because different mistakes matter differently, and one score can hide serious failure modes." },
  ]}
/>

<SubsectionHeader emoji="‚úÖ">
  After this section you should be able to:
</SubsectionHeader>

1. Explain why representation choices change what a model can learn
2. Explain what breaks when labels, groups, or context are missing from data
3. Explain the trade off between simple features and richer embeddings

---

## Supervised and unsupervised learning

<SectionHeader variant="content" emoji="üéì" id="learning-paradigms">
  Supervised and unsupervised learning
</SectionHeader>

<SectionProgressToggle courseId="ai" levelId="foundations" sectionId="ai-foundations-learning-paradigms" />

When we say a model learns, we mean it changes its internal settings so it can make better guesses. It is not learning like a person learns. It is closer to practice. You show examples, it adjusts, and it gets less wrong over time.

<GlossaryTip term="training data">
Training data is the set of examples you use to teach the model. It is the practice material, not the final exam.
</GlossaryTip>

<GlossaryTip term="supervised learning">
Supervised learning is when a model learns using examples that already have the correct answer attached.
</GlossaryTip>

In supervised learning, you give the model an input and an answer. The model tries to guess the answer, then it is corrected. Over many examples, it learns a pattern that can generalise to new cases.

Email spam filtering is a classic supervised example. You have emails, and you have labels like spam and not spam. Image classification is another. You have images, and you have labels like cat, dog, or receipt. House prices are supervised too, but the answer is a number. The same pattern applies. Inputs in, answer attached, model learns to predict.

There are two common supervised shapes.

<GlossaryTip term="classification">
Classification is predicting a category. For example spam or not spam.
</GlossaryTip>

<GlossaryTip term="regression">
Regression is predicting a number. For example a house price.
</GlossaryTip>

The difference matters because the mistakes feel different. A wrong category can block a real email. A wrong price can cost real money.

<GlossaryTip term="unsupervised learning">
Unsupervised learning is when a model looks for structure in data without being told the correct answers.
</GlossaryTip>

Instead of asking "what is the right label", you ask "what patterns exist". This is useful when labels are missing, expensive, or not even well defined.

Grouping customers by behaviour is a common unsupervised example. You might discover that one group buys weekly and another group buys once a year. Topic discovery in documents is another. You might find clusters of themes in support tickets without anyone labelling them by hand. Anomaly detection is a third. You look for unusual behaviour that might signal fraud or intrusion.

Unsupervised learning is harder to evaluate because there is no single correct answer waiting in a spreadsheet. If you change your settings, the groupings can change. Sometimes both results are reasonable. You have to judge usefulness, not just score points.

Imagine a bank clustering transactions to find ‚Äúnormal‚Äù behaviour. If the system learns that weekend spending is ‚Äúunusual‚Äù for a certain group, it might flag normal customers as fraud. Unsupervised results still need human judgement and context.

In practice, teams use clustering to create segments and then make decisions based on those segments. That means errors in the clustering can become policy, pricing, or access decisions. Treat cluster labels as hypotheses, not truth.

Here are a few beginner misconceptions to avoid. First, more data is not always better data. If it is biased or messy, you scale the problem. Second, unsupervised learning is not a free shortcut. It still needs careful interpretation. Third, a model learning a pattern does not mean it understands a reason. It means it found a shortcut that worked on the training data.

<DiagramBlock title="Two ways models learn from data" subtitle="Supervised has answers. Unsupervised searches for structure.">
  <div className="text-xs sm:text-sm leading-5 space-y-2">
    <div>Supervised: inputs + labels -> predict a known outcome</div>
    <div>Unsupervised: inputs only -> group, compress, or flag unusual patterns</div>
    <div>Supervised output: category or number</div>
    <div>Unsupervised output: clusters, topics, or anomaly scores</div>
  </div>
</DiagramBlock>

<ToolCard
  id="tiny-classifier-lab"
  title="Train a tiny classifier"
  description="Adjust how much training data you have and how noisy it is. Notice how it changes expected accuracy and stability."
>
  <TinyClassifierLabTool />
</ToolCard>

<QuizBlock
  id="ai-foundations-learning-quiz"
  courseId="ai"
  levelId="foundations"
  sectionId="ai-foundations-learning-paradigms"
  title="Quick check. Supervised and unsupervised learning"
  questions={[
    { q: "In plain language, what does learning mean for a model", a: "It adjusts its internal settings so it makes better guesses from examples." },
    { q: "What is training data", a: "The examples used to teach the model, not the final test." },
    { q: "What makes learning supervised", a: "Examples include the correct answer the model should learn to predict." },
    { q: "What makes learning unsupervised", a: "Examples have no answers, so the model looks for patterns or structure." },
    { q: "Give one supervised example", a: "Spam filtering, image classification, or house price prediction." },
    { q: "Give one unsupervised example", a: "Grouping customers, topic discovery, or anomaly detection." },
    { q: "What is classification", a: "Predicting a category like spam or not spam." },
    { q: "What is regression", a: "Predicting a number like a house price." },
    { q: "Why is unsupervised learning harder to evaluate", a: "There is no single correct answer, so usefulness is judged by interpretation and context." },
    { q: "Name one common beginner misconception", a: "Thinking more data automatically means better results, or that unsupervised learning removes the need for judgement." },
    { q: "When might you prefer unsupervised learning", a: "When labels are missing or you want to discover structure like clusters or anomalies." },
    { q: "What is a practical risk with clustering results", a: "They can be over-interpreted as truth when they are just one useful grouping choice." },
  ]}
/>

<SubsectionHeader emoji="‚úÖ">
  After this section you should be able to:
</SubsectionHeader>

1. Explain when supervised learning is appropriate and what it optimises for
2. Explain when unsupervised learning is useful and why evaluation is harder
3. Explain what breaks when people treat model outputs as understanding

---

## Responsible AI basics and limitations

<SectionHeader variant="content" emoji="‚öñÔ∏è" id="responsible-ai">
  Responsible AI basics and limitations
</SectionHeader>

<SectionProgressToggle courseId="ai" levelId="foundations" sectionId="ai-foundations-responsible-ai-basics" />

AI systems can cause harm even when everybody is trying to do the right thing. The harm is not always dramatic. Sometimes it is quiet and personal. Someone is incorrectly flagged as suspicious. Someone does not get offered an opportunity. Someone is pushed into a bubble of content that makes them angrier and more certain.

The core reason is simple. Models learn patterns, not truth. They learn what tends to follow what in the data they were given. They do not know what is fair. They do not know what is lawful. They do not know what is kind. They only know what was rewarded during training.

<GlossaryTip term="bias">
Bias is when a system consistently produces unfair or skewed outcomes because of the data or assumptions it was built on.
</GlossaryTip>

Bias can enter through the data, through the labels, and through the choices we make. If your training data under represents some faces, facial recognition errors show up first in those groups. If your labels reflect past human decisions, you teach the model those decisions as if they were objective truth. If you optimise only for speed, you often trade away care.

In a real system, a ‚Äúsmall‚Äù bias can become a big harm because automation runs every day. Imagine an AI triage tool that consistently underestimates risk for one group. Even if the average performance looks fine, the harm concentrates on real people.

In practice, the fix is not only better metrics. The fix is process: clear accountability, human review for high impact cases, and monitoring that triggers action when outcomes drift or complaints rise.

<GlossaryTip term="fairness">
Fairness is the practice of reducing avoidable harm across groups and being able to justify outcomes.
</GlossaryTip>

Fairness is not a single magic number. It is a set of decisions. What harm do we want to prevent. Which groups matter in this context. What trade offs are acceptable. It belongs with humans, not only with metrics.

<GlossaryTip term="explainability">
Explainability is the ability to give a clear, human understandable reason for why a model produced an output.
</GlossaryTip>

Explainability matters most when decisions affect real lives. An automated decision system that cannot explain itself becomes hard to challenge. People then learn to distrust the whole process, or worse, they stop trying to challenge it at all.

Bigger models do not automatically mean better or safer. They can be more capable and still be more confusing. They can also be more expensive to run, which encourages shortcuts. If it costs too much to monitor and evaluate, teams quietly stop doing it.

<SubsectionHeader emoji="üéØ">
  What this optimises for
</SubsectionHeader>

1. Automation speed and consistency for low risk tasks
2. Scaled assistance when humans have time and context

<SubsectionHeader emoji="‚ö†Ô∏è">
  What this makes harder
</SubsectionHeader>

1. Accountability if oversight is vague
2. Safe rollback when models drift or fail

AI confidence is not the same as correctness. A model can be very confident and still be wrong. This shows up most painfully when people over trust AI answers. A model that sounds fluent can still invent details. It is good at sounding plausible. It is not automatically good at being true.

<GlossaryTip term="hallucination">
A hallucination is a confident answer that is not grounded in the input or reality.
</GlossaryTip>

This is why human judgement must stay in control. Use AI to help you think, draft, or explore. Do not let it quietly become the decision maker for high impact outcomes. If a system can deny a benefit, flag a person, or change a life, it needs clear oversight and a way to appeal.

<GlossaryTip term="model drift">
Model drift is when real world data changes over time so the model performs worse than it did during evaluation.
</GlossaryTip>

Drift is not rare. It is normal. People change behaviour. Fraud patterns adapt. Language evolves. Even a simple recommendation system can drift into a bubble because the system is changing the data it then learns from. You cannot evaluate once and declare victory.

<DiagramBlock title="The AI lifecycle and where risks appear" subtitle="Risk shows up at every step. Oversight wraps the loop.">
  <div className="text-xs sm:text-sm leading-5 space-y-2">
    <div>Data collection -> Training -> Evaluation</div>
    <div>Deployment -> Monitoring -> Improvement</div>
    <div className="text-slate-700">Human oversight wraps the lifecycle with review, policy, and clear accountability.</div>
  </div>
</DiagramBlock>

<ToolCard
  id="ai-risk-scenarios"
  title="Spot AI risks in everyday scenarios"
  description="Read short AI stories and practice identifying bias, safety risks and over trust in model outputs."
>
  <ResponsibleAIPlannerTool />
</ToolCard>

<QuizBlock
  id="ai-foundations-responsible-quiz"
  courseId="ai"
  levelId="foundations"
  sectionId="ai-foundations-responsible-ai-basics"
  title="Quick check. Responsible AI basics and limitations"
  questions={[
    { q: "Why can AI cause harm even when well intentioned", a: "Models learn patterns from data and can scale mistakes, bias, and bad assumptions." },
    { q: "What does it mean that models learn patterns, not truth", a: "They learn statistical regularities, not what is correct, fair, or lawful." },
    { q: "Name two places bias can enter an AI system", a: "Through data coverage, labels, or the assumptions and objectives chosen." },
    { q: "Why are facial recognition errors a fairness issue", a: "Error rates can be higher for under represented groups, causing unequal harm." },
    { q: "Why is bigger not automatically safer", a: "More capability can come with more opacity, cost, and operational shortcuts." },
    { q: "Why is AI confidence not the same as correctness", a: "A model can be confident and still be wrong because confidence is not a truth check." },
    { q: "What is a hallucination", a: "A confident output that is not grounded in the input or reality." },
    { q: "Why is evaluation not a one time step", a: "Real world conditions change and models can drift, so performance must be monitored." },
    { q: "What is model drift", a: "When data or behaviour changes over time so the model performs worse than before." },
    { q: "When must humans stay in control", a: "When decisions are high impact and affect rights, safety, or access to opportunities." },
    { q: "What is a good default for high impact decisions", a: "Human review with clear accountability, appeal paths, and monitoring." },
    { q: "What is one governance artifact teams should keep", a: "A model card, decision log, or documented risk and monitoring plan." },
  ]}
/>

<SubsectionHeader emoji="‚úÖ">
  After this section you should be able to:
</SubsectionHeader>

1. Explain why models do not understand intent or truth and what breaks if you trust them blindly
2. Explain why evaluation and monitoring are continuous, not a one time step
3. Explain the trade off between AI automation and human oversight in high impact decisions

<BodyText>
  You now have the foundations to talk clearly about data, learning, and limits. Intermediate will build on this with evaluation, deployment, and system thinking. Foundations are about how you think, not which tool you use.
</BodyText>

---

<PageNav prevHref="/ai" prevLabel="AI course overview" nextHref="/ai/intermediate" nextLabel="Intermediate" showTop showBottom />
