---
title: "AI Advanced"
slug: "advanced"
courseId: "ai"
levelId: "advanced"
summary: "Production AI systems, architecture decisions, and failure modes at scale."
estimatedHours: 12
stepIndex: 2
---

import GlossaryTip from "@/components/notes/GlossaryTip"
import ToolCard from "@/components/notes/ToolCard"
import QuizBlock from "@/components/notes/QuizBlock"
import SectionProgressToggle from "@/components/notes/SectionProgressToggle"
import LevelProgressBar from "@/components/course/LevelProgressBar"
import CPDTracker from "@/components/CPDTracker"
import DiagramBlock from "@/components/DiagramBlock"
import PageNav from "@/components/notes/PageNav"
import { aiSectionManifest } from "@/lib/aiSections"

# AI Advanced

<LevelProgressBar courseId="ai" levelId="advanced" sectionIds={aiSectionManifest.advanced} />

<CPDTracker courseId="ai" levelId="advanced" estimatedHours={12} />

<PageNav prevHref="/ai/intermediate" prevLabel="Intermediate" nextHref="/ai/summary" nextLabel="Summary and games" showTop />

## AI systems and model architectures

<SectionProgressToggle
  courseId="ai"
  levelId="advanced"
  sectionId="ai-advanced-systems-and-architectures"
/>

A model is a component that maps inputs to outputs. An AI system is the full product around it: interfaces, data flow, guardrails, monitoring, and the operational process that keeps outputs useful. At scale, system design usually dominates the outcome. The same model can look brilliant or useless depending on how it is integrated.

Models rarely operate alone because real inputs are messy and real decisions have constraints. You need routing, caching, authentication, permissions, and careful handling of failures. You also need data sources the system can trust. Without that, the model becomes a confident narrator of whatever it last saw.

The moment you put a model behind an API, you are doing inference.

<GlossaryTip term="inference">
Inference is running the model to produce outputs from inputs.
</GlossaryTip>

In production, inference has strict budgets. You have cost budgets, latency budgets, and reliability budgets. Those budgets shape architecture more than a training run does.

One pattern is batch inference. You run predictions on a schedule, store results, and serve them fast later. This works well for things like nightly fraud scoring, content tagging, or pricing suggestions. The trade off is freshness. If the world changes at noon, your results might not catch up until tomorrow.

Another pattern is real time inference. Requests hit an API, the system calls the model, and the result returns immediately. This is common in ranking, moderation, and interactive assistants. Here latency matters.

<GlossaryTip term="latency">
Latency is the time it takes to return a result after a request.
</GlossaryTip>

Latency is not just performance vanity. It changes user behaviour and it changes system load. A slow model can create backlogs, timeouts, and cascading failures.

A third pattern is retrieval augmented systems. You keep a data store of documents, records, or snippets, retrieve relevant pieces at request time, then feed them into the model. This is often called retrieval augmented generation.

<GlossaryTip term="retrieval augmented generation">
Retrieval augmented generation is using retrieved external data as context so the model can respond with fresher, more grounded outputs.
</GlossaryTip>

The architecture shifts the problem from "make the model smarter" to "make the data pipeline reliable". Retrieval quality, permissions, and content freshness become the main levers.

At scale, orchestration and data flow matter more than raw accuracy.

<GlossaryTip term="orchestration">
Orchestration is coordinating steps and services so the right data and tools are used at the right time.
</GlossaryTip>

If you cannot trace what data was used, what model version ran, and why a decision happened, you cannot operate the system safely. Good architecture makes failures visible, limits blast radius, and makes improvements repeatable.

<DiagramBlock title="Typical production AI system" subtitle="Separation of concerns keeps systems operable.">
  <div className="text-xs sm:text-sm leading-5 space-y-2">
    <div>User input -> API layer (auth, routing, validation)</div>
    <div>API layer -> Model inference (bounded compute)</div>
    <div>API layer to and from Data store (retrieval, features, permissions)</div>
    <div>Monitoring and logging (latency, errors, quality signals)</div>
  </div>
</DiagramBlock>


<ToolCard
  id="ai-architecture-explorer"
  title="Explore AI system architectures"
  description="Toggle between batch, real time and retrieval based AI systems to see how data and models interact."
/>

<QuizBlock
  id="ai-advanced-systems-quiz"
  title="Check your understanding of AI system architecture"
  courseId="ai"
  levelId="advanced"
  sectionId="ai-advanced-systems-and-architectures"
  questions={[
    { q: "What is the difference between a model and an AI system", a: "A model maps inputs to outputs, an AI system includes the product around it like data flow, APIs, guardrails, and operations." },
    { q: "Why do models rarely operate alone in production", a: "Real inputs and decisions need routing, permissions, caching, fallbacks, and trusted data sources." },
    { q: "What does inference mean", a: "Running the model to produce outputs from inputs." },
    { q: "Why is latency a design constraint", a: "It affects user experience and can cause timeouts, backlogs, and cascading failures." },
    { q: "When is batch inference a good fit", a: "When predictions can be computed on a schedule and served later, like nightly scoring or tagging." },
    { q: "What is a key trade off of batch inference", a: "Results can be stale when the world changes between runs." },
    { q: "What does retrieval augmented generation add to a system", a: "It pulls relevant external data into context so outputs can be fresher and more grounded." },
    { q: "Why does orchestration exist", a: "To coordinate steps and data so the right tools and context are used reliably and safely." },
    { q: "Why can architecture matter more than accuracy at scale", a: "Because operability, traceability, permissions, and failure handling determine whether the system stays useful and safe." },
  ]}
/>

## Scaling, cost and reliability in AI systems

<SectionProgressToggle
  courseId="ai"
  levelId="advanced"
  sectionId="ai-advanced-scaling-cost-reliability"
/>

Scaling is not a single knob. It is a set of constraints you discover when traffic hits. The first surprise is that models are only one part of the system. Data fetches, feature generation, retrieval calls, and post processing often become the bottleneck before the model does.

When people say scale, they usually mean capacity. <GlossaryTip term="horizontal scaling">
Horizontal scaling is adding more instances of a service to handle more load.
</GlossaryTip> Vertical scaling is making a single instance bigger, like more CPU, more GPU, or more memory. In AI systems, vertical scaling buys you headroom per request. Horizontal scaling buys you throughput and fault tolerance. You usually need both, but you should start by making the service stateless so scaling is possible.

<GlossaryTip term="stateless service">
A stateless service does not store user or session state in memory between requests.
</GlossaryTip>

Stateless inference services are easier to deploy and easier to recover. You can restart a bad instance without losing user state. You can spread load across instances without sticky sessions. If you need state, push it to a data store with clear ownership and clear timeouts.

Two levers help before you add more compute: caching and batching. Caching is serving repeated requests from memory or a fast store. Batching is grouping multiple requests into one model call so you use hardware more efficiently. Both reduce cost, but both can change behaviour. Caching can serve stale answers. Batching can increase latency for the first request in the batch. These are product decisions, not just infrastructure choices.

Scaling data can hurt more than scaling models. Retrieval adds network calls, index lookups, and permission checks. Feature pipelines add joins, transformations, and schema drift. If your data path is slow, scaling the model will not fix it. You are just making the wrong part faster.

Cost shows up as cost per request, and the hidden multiplier is how many steps your system takes per request. One model call is expensive. A model call plus retrieval, plus reranking, plus embeddings, plus retries can turn a single user action into a small workflow. The user sees one answer. You pay for the whole graph.

Embeddings and retrieval have their own cost curve. Embeddings can be expensive to compute and expensive to store at scale. Retrieval can be cheap per query and still expensive overall because it runs for every request. The easiest way to lose money is to add a pipeline step that runs always, even when it only helps sometimes.

Overprovisioning is the second way to lose money. Keeping GPUs warm for peak traffic feels safe, but it can make costs explode if the peak is rare. Autoscaling helps, but cold starts can hurt latency. This is why you trade off accuracy, latency, and cost as a trio. A slower model might be more accurate. A faster model might be cheaper. A cached answer might be good enough most of the time.

Reliability is where the real lessons are. Models fail in boring ways. They time out. They return empty output. They return something plausible and wrong. They get slower under load. They hit rate limits from an upstream provider. Partial failures are normal in distributed systems, so assume they will happen.

Retries are useful, but they are dangerous under load. If every request times out and you retry immediately, you double traffic into a system that is already struggling. This is why you need a retry policy.

<GlossaryTip term="retry policy">
A retry policy defines when to retry, how many times, and how long to wait between attempts.
</GlossaryTip>

A good retry policy uses backoff and caps. It retries only when it makes sense, like transient network issues, not when a model is overloaded. It also needs per request budgets, so retries do not turn a slow failure into an outage.

The practical goal is graceful degradation.

<GlossaryTip term="graceful degradation">
Graceful degradation means the system still provides a safe, simpler experience when parts fail.
</GlossaryTip>

That can mean switching to a smaller model, skipping retrieval, returning a cached answer, or showing a clear fallback message. The key is that failure is expected, not exceptional. Your architecture should make the fallback path explicit and testable.

<DiagramBlock title="Scaling and failure in AI systems" subtitle="Assume load and failure. Design the cache and fallback paths early.">
  <div className="text-xs sm:text-sm leading-5 space-y-2">
    <div>Load increases: more requests and longer queues</div>
    <div>Cache layer absorbs traffic: repeats served fast</div>
    <div>Model service scales out: more instances handle throughput</div>
    <div>Failure happens: timeouts, rate limits, partial dependency loss</div>
    <div>Fallback path activates: smaller model, cached answer, or safe message</div>
  </div>
</DiagramBlock>

<ToolCard
  id="ai-scaling-simulator"
  title="Simulate AI system load"
  description="Adjust traffic, latency and failure rates to see how cost and reliability change."
/>

<QuizBlock
  id="ai-advanced-scaling-quiz"
  title="Check your understanding of scaling and reliability"
  courseId="ai"
  levelId="advanced"
  sectionId="ai-advanced-scaling-cost-reliability"
  questions={[
    { q: "What is horizontal scaling", a: "Adding more service instances to handle more load." },
    { q: "Why do stateless services scale more easily", a: "Because any instance can handle any request and instances can restart without losing user state." },
    { q: "Name two levers that can reduce cost before adding more compute", a: "Caching and batching." },
    { q: "Why can scaling data hurt more than scaling models", a: "Because retrieval and feature pipelines can add slow network and transformation work that becomes the real bottleneck." },
    { q: "What is a common hidden cost driver in AI systems", a: "Extra per request steps like retrieval, embeddings, reranking, and retries." },
    { q: "Why can retries be dangerous under load", a: "They add traffic to a system that is already struggling and can turn slowdowns into outages." },
    { q: "What should a retry policy define", a: "When to retry, how many times, and how long to wait between attempts." },
    { q: "What is graceful degradation", a: "Providing a safe, simpler experience when parts fail, such as a fallback model or cached answer." },
    { q: "Why should failure be treated as expected in architecture", a: "Because distributed dependencies and model services fail in normal ways, so fallback paths must be explicit and testable." },
  ]}
/>

## Evaluation, monitoring and governance in production AI

<SectionProgressToggle
  courseId="ai"
  levelId="advanced"
  sectionId="ai-advanced-evaluation-monitoring-governance"
/>

Evaluation in production is not a single score. It is a series of checks that answer one question: does the system still help the business without creating unacceptable harm. That requires measurement before launch, after launch, and while the world changes.

Offline evaluation is what you do on datasets you control. It is useful for comparing versions and catching obvious regressions. It also has blind spots. Offline data is usually cleaner than reality, and it rarely contains the full cost of mistakes. Online evaluation is what you do in the live system, where users, latency, and edge cases are real. A model that looks strong offline can still fail online if it changes user behaviour or breaks workflows.

Accuracy alone is not enough because the cost of mistakes is not symmetric. A fraud system that misses fraud can be catastrophic. A moderation system that over blocks can silence legitimate users. In these cases you reach for precision and recall, then connect them to real outcomes like chargebacks, review workload, or user churn.

<GlossaryTip term="precision">
Precision is how often a predicted positive is truly positive.
</GlossaryTip>

<GlossaryTip term="recall">
Recall is how many of the true positives the system successfully catches.
</GlossaryTip>

Even strong metrics are not stable. You need to monitor performance over time because production data is a moving target. The metric you track depends on the system, but the habit is the same: measure, investigate, and learn. If you cannot explain why a metric moved, you cannot fix it.

Monitoring is the boring work that saves you. Start with inputs. Are you seeing new categories, missing fields, unusual ranges, or sudden format changes. Then monitor outputs. Are scores shifting, are confidence values drifting upward, are certain groups being flagged more often. Finally monitor system health: latency, error rate, and rate limiting. If the system is slow, it will change user behaviour and it will change your data.

Alerting is where good teams become noisy teams. Too many alerts create alert fatigue, and then real problems are ignored. You want alerts that are actionable, tied to clear owners, and paired with a playbook. False positives in monitoring are not harmless. They burn trust and time.

Drift is the quiet killer. Data drift is when inputs change. Concept drift is when the meaning of the target changes, even if inputs look similar. A credit scoring model can see stable features while repayment behaviour changes during a downturn. A moderation model can see similar text while norms and tactics change.

<GlossaryTip term="data drift">
Data drift is when the input data distribution changes over time.
</GlossaryTip>

<GlossaryTip term="concept drift">
Concept drift is when the relationship between inputs and the correct outcome changes.
</GlossaryTip>

Retraining schedules matter because drift does not wait for your roadmap. Some systems need periodic retraining. Others need trigger based retraining when drift crosses a threshold. Either way, you should treat retraining like a release, with the same discipline: evaluation, rollout, rollback, and audit trails.

This is where governance stops being paperwork and becomes operations. Someone must own the model and the system around it. Decisions about thresholds, fallbacks, and acceptable harm are product and risk decisions, not just ML decisions.

<GlossaryTip term="model governance">
Model governance is the process and ownership that controls how models are documented, deployed, monitored, and changed.
</GlossaryTip>

Good governance includes documentation and traceability. You want to know which data, which features, which model version, and which thresholds produced an outcome. Human oversight is part of the design. It needs authority, not just review. And you need a shutdown plan. If the system is causing harm or you cannot understand its behaviour, you stop it, switch to a safe fallback, and investigate.

<DiagramBlock title="AI system lifecycle in production" subtitle="Governance surrounds the lifecycle, from data to retirement.">
  <div className="text-xs sm:text-sm leading-5 space-y-2">
    <div>Data collection: quality checks, access rules, consent</div>
    <div>Training: versioning, documentation, repeatability</div>
    <div>Deployment: rollout, fallbacks, human oversight</div>
    <div>Monitoring: performance, drift, latency, error rates</div>
    <div>Retraining or retirement: update safely or shut down</div>
    <div>Governance: ownership, traceability, review, incident response</div>
  </div>
</DiagramBlock>

<ToolCard
  id="ai-monitoring-lab"
  title="Explore model monitoring signals"
  description="Inspect metrics like accuracy, drift and latency to decide when a model needs attention."
/>

<QuizBlock
  id="ai-advanced-monitoring-quiz"
  title="Check your understanding of monitoring and governance"
  courseId="ai"
  levelId="advanced"
  sectionId="ai-advanced-evaluation-monitoring-governance"
  questions={[
    { q: "What is the difference between offline and online evaluation", a: "Offline uses controlled datasets to compare versions, online measures behaviour in the live system with real users and constraints." },
    { q: "Why is accuracy alone not enough in many systems", a: "Because different mistakes have different costs and accuracy can hide harmful trade offs." },
    { q: "In plain terms, what does precision measure", a: "When the system flags something, how often it is truly a positive." },
    { q: "In plain terms, what does recall measure", a: "Of the real positives, how many the system successfully catches." },
    { q: "Name three monitoring areas in production AI", a: "Inputs, outputs, and system health like latency and error rates." },
    { q: "What is alert fatigue and why is it dangerous", a: "Too many alerts cause people to ignore them, so real issues get missed." },
    { q: "What is data drift", a: "Inputs change over time so the model sees a different distribution than before." },
    { q: "What is concept drift", a: "The link between inputs and the correct outcome changes even if inputs look similar." },
    { q: "Why do retraining schedules matter", a: "Because drift can degrade performance and retraining needs to be planned like a controlled release." },
    { q: "When should a system be shut down", a: "When it is causing harm, behaving unpredictably, or you cannot operate it safely and explain what it is doing." },
  ]}
/>

<PageNav prevHref="/ai/intermediate" prevLabel="Intermediate" nextHref="/ai/summary" nextLabel="Summary and games" showBottom />


