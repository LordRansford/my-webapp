{"pageProps":{"source":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    a: \"a\",\n    h1: \"h1\",\n    hr: \"hr\",\n    li: \"li\",\n    p: \"p\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  }, {AIExamplesExplorerTool, BodyText, CPDTracker, Callout, DiagramBlock, GlossaryTip, LevelProgressBar, PageNav, QuizBlock, ResponsibleAIPlannerTool, SectionHeader, SectionProgressToggle, SubsectionHeader, TinyClassifierLabTool, ToolCard, VectorVisualiserTool} = _components;\n  if (!AIExamplesExplorerTool) _missingMdxReference(\"AIExamplesExplorerTool\", true);\n  if (!BodyText) _missingMdxReference(\"BodyText\", true);\n  if (!CPDTracker) _missingMdxReference(\"CPDTracker\", true);\n  if (!Callout) _missingMdxReference(\"Callout\", true);\n  if (!DiagramBlock) _missingMdxReference(\"DiagramBlock\", true);\n  if (!GlossaryTip) _missingMdxReference(\"GlossaryTip\", true);\n  if (!LevelProgressBar) _missingMdxReference(\"LevelProgressBar\", true);\n  if (!PageNav) _missingMdxReference(\"PageNav\", true);\n  if (!QuizBlock) _missingMdxReference(\"QuizBlock\", true);\n  if (!ResponsibleAIPlannerTool) _missingMdxReference(\"ResponsibleAIPlannerTool\", true);\n  if (!SectionHeader) _missingMdxReference(\"SectionHeader\", true);\n  if (!SectionProgressToggle) _missingMdxReference(\"SectionProgressToggle\", true);\n  if (!SubsectionHeader) _missingMdxReference(\"SubsectionHeader\", true);\n  if (!TinyClassifierLabTool) _missingMdxReference(\"TinyClassifierLabTool\", true);\n  if (!ToolCard) _missingMdxReference(\"ToolCard\", true);\n  if (!VectorVisualiserTool) _missingMdxReference(\"VectorVisualiserTool\", true);\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.h1, {\n      id: \"ai-foundations\",\n      children: _jsx(_components.a, {\n        href: \"#ai-foundations\",\n        children: \"AI Foundations\"\n      })\n    }), \"\\n\", _jsx(LevelProgressBar, {\n      courseId: \"ai\",\n      levelId: \"foundations\",\n      sectionIds: aiSectionManifest.foundations\n    }), \"\\n\", _jsx(CPDTracker, {\n      courseId: \"ai\",\n      levelId: \"foundations\",\n      estimatedHours: 8\n    }), \"\\n\", _jsx(BodyText, {\n      children: _jsx(_components.p, {\n        children: \"These notes are a calm path into AI. I focus on meaning first, numbers second, and practice always. The goal is not buzzwords. The goal is to build judgement you can use when you meet real systems.\"\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(SectionHeader, {\n      variant: \"content\",\n      emoji: \"ðŸ§ \",\n      id: \"what-is-ai\",\n      children: _jsx(_components.p, {\n        children: \"What AI is and why it matters now\"\n      })\n    }), \"\\n\", _jsx(SectionProgressToggle, {\n      courseId: \"ai\",\n      levelId: \"foundations\",\n      sectionId: \"ai-foundations-what-is-ai\"\n    }), \"\\n\", _jsx(Callout, {\n      variant: \"concept\",\n      children: _jsx(_components.p, {\n        children: \"AI is a way of learning patterns from data so a system can make predictions, rank options, or automate decisions.\"\n      })\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"When people say AI, they often mean a system that takes input, applies a learned pattern, and produces an output. The learned pattern is the \", _jsx(GlossaryTip, {\n        term: \"model\",\n        children: \"A function learned from data that maps inputs to outputs.\"\n      }), \". The act of learning that pattern is \", _jsx(GlossaryTip, {\n        term: \"training\",\n        children: \"The process of fitting a model to data so it can learn patterns.\"\n      }), \". Using the trained model to produce results is \", _jsx(GlossaryTip, {\n        term: \"inference\",\n        children: \"Running a trained model to produce predictions on new inputs.\"\n      }), \".\"]\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"The difference between training and inference matters because the risks are different. Training is where you bake in assumptions from the \", _jsx(GlossaryTip, {\n        term: \"dataset\",\n        children: \"A collection of examples used to train and evaluate a model.\"\n      }), \". Inference is where the model meets reality. If reality changes, the model can behave badly even if training looked perfect.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"AI is powerful because it can learn patterns too complex to write as hand made rules. It is also fragile because it can learn the wrong pattern. A model can look clever while failing quietly. The skill is to ask what it is really using as evidence.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"AI matters now because systems touch decisions that used to be manual. Hiring screens, fraud checks, support routing, and medical triage all use models to move faster. That speed is useful, but it can also amplify mistakes at scale. This is why foundations matter. I need to know what the model is doing before I trust it.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"My calm view on hype is this. AI is not magic. It is applied statistics plus engineering. It will keep changing how work is done, and it will keep producing failures that look silly in hindsight. If you learn the basics, you can use it well without believing the marketing.\"\n    }), \"\\n\", _jsx(DiagramBlock, {\n      title: \"Rules based software vs model based systems\",\n      subtitle: \"Rules are explicit. Models are learned from data.\",\n      children: _jsxs(\"div\", {\n        className: \"text-xs sm:text-sm leading-5 space-y-2\",\n        children: [_jsx(\"div\", {\n          children: \"Rules: if condition then action\"\n        }), _jsx(\"div\", {\n          children: \"Model: input -> model -> output\"\n        }), _jsx(\"div\", {\n          children: \"Training builds the model. Inference uses it.\"\n        }), _jsx(\"div\", {\n          children: \"Rules are easier to explain. Models need monitoring and care.\"\n        })]\n      })\n    }), \"\\n\", _jsx(ToolCard, {\n      id: \"ai-examples-explorer\",\n      title: \"Explore simple AI examples\",\n      description: \"Toggle between examples like spam filters, recommendation systems and chat assistants, and see what each one takes as input and gives as output.\",\n      children: _jsx(AIExamplesExplorerTool, {})\n    }), \"\\n\", _jsx(QuizBlock, {\n      id: \"ai-foundations-what-is-ai-quiz\",\n      courseId: \"ai\",\n      levelId: \"foundations\",\n      sectionId: \"ai-foundations-what-is-ai\",\n      title: \"Quick check: what is AI\",\n      questions: [{\n        q: \"What is a model in AI\",\n        a: \"A learned function that maps inputs to outputs.\"\n      }, {\n        q: \"What is training\",\n        a: \"Fitting a model to data so it learns patterns.\"\n      }, {\n        q: \"What is inference\",\n        a: \"Using a trained model to make predictions on new inputs.\"\n      }, {\n        q: \"Give one everyday AI example\",\n        a: \"Spam filtering, recommendations, or fraud detection.\"\n      }, {\n        q: \"Why does AI matter now\",\n        a: \"It scales decisions that used to be manual and can amplify impact.\"\n      }, {\n        q: \"What is one risk of AI at scale\",\n        a: \"Mistakes spread faster and affect more people.\"\n      }, {\n        q: \"What does AI output usually look like\",\n        a: \"A label, score, or ranked list.\"\n      }, {\n        q: \"What is generalisation\",\n        a: \"When a model performs well on new data, not just the training data.\"\n      }]\n    }), \"\\n\", _jsx(SubsectionHeader, {\n      emoji: \"âœ…\",\n      children: _jsx(_components.p, {\n        children: \"After this section you should be able to:\"\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Explain what a model is and why it exists in a system\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Explain what breaks when training data and real inputs diverge\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Explain the trade off between automation speed and human judgement\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(SectionHeader, {\n      variant: \"content\",\n      emoji: \"ðŸ“Š\",\n      id: \"data-and-representation\",\n      children: _jsx(_components.p, {\n        children: \"Data and representation\"\n      })\n    }), \"\\n\", _jsx(SectionProgressToggle, {\n      courseId: \"ai\",\n      levelId: \"foundations\",\n      sectionId: \"ai-foundations-data-and-representation\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"In AI, the word \", _jsx(GlossaryTip, {\n        term: \"data\",\n        children: \"\\nData is recorded observations about the world. It is what the model learns from, not what we wish was true.\\n\"\n      }), \" sounds fancy, but it is usually boring. It is clicks, purchases, support tickets, photos, sensor readings, and text. Data always comes with context. Where did it come from. Who produced it. What is missing. What was measured badly. If you ignore that, you build a confident model on shaky ground.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Some data is structured. That means it fits neatly into rows and columns. Think customer age, number of failed logins, or time since last password reset. Other data is unstructured. That means it looks like raw text, images, audio, or long logs. It still has structure, but you have to extract it.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"To train a model, we usually separate inputs from the answer we want. A \", _jsx(GlossaryTip, {\n        term: \"feature\",\n        children: \"\\nA feature is a measurable piece of information about something. For example the length of an email or the number of failed login attempts.\\n\"\n      }), \" is an input signal. A \", _jsx(GlossaryTip, {\n        term: \"label\",\n        children: \"\\nA label is the correct answer for training. For example spam or not spam, or refund needed or not.\\n\"\n      }), \" is the outcome we want the model to learn to predict.\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Models cannot understand raw text or images the way humans do. They do not see meaning. They see numbers. If you give a model a photo, it will be turned into numbers first. If you give it an email, it will be turned into numbers first. The model learns patterns in those numbers.\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"The simplest numeric form is a \", _jsx(GlossaryTip, {\n        term: \"vector\",\n        children: \"\\nA vector is a list of numbers that represents an input. The numbers are chosen so the model can work with them.\\n\"\n      }), \". For text, we first break it into a \", _jsx(GlossaryTip, {\n        term: \"token\",\n        children: \"\\nA token is a small chunk of text used as the unit for language models. It might be a word or part of a word.\\n\"\n      }), \". Then we map those tokens into an \", _jsx(GlossaryTip, {\n        term: \"embedding\",\n        children: \"\\nAn embedding is a numeric vector that tries to place similar items near each other in number space.\\n\"\n      }), \".\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The intuition is simple. If two pieces of text are used in similar ways, they often end up with similar numbers. A model can then treat closeness as a hint that the meaning is related. It is not perfect. It is a useful shortcut.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Bad data creates bad models. If the labels are wrong, the model learns the wrong lesson. If the data is missing whole groups of people, the model will fail on those groups. If the data reflects old behaviour, the model will struggle when the world changes. This is why data work is not busywork. It is the foundation.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Splitting data matters because we want honest feedback. Training data is what the model learns from. Validation data is what you use to make choices during building. Test data is the final check you keep separate until the end. If you test on the same data you trained on, you are grading your own homework with the answer sheet open.\"\n    }), \"\\n\", _jsx(DiagramBlock, {\n      title: \"From raw data to numbers a model can learn from\",\n      subtitle: \"Turn messy inputs into numeric representation.\",\n      children: _jsxs(\"div\", {\n        className: \"text-xs sm:text-sm leading-5 space-y-2\",\n        children: [_jsx(\"div\", {\n          children: \"Raw text or images\"\n        }), _jsx(\"div\", {\n          children: \"Cleaning and preparation\"\n        }), _jsx(\"div\", {\n          children: \"Features chosen from the data\"\n        }), _jsx(\"div\", {\n          children: \"Numeric representation as vectors and embeddings\"\n        }), _jsx(\"div\", {\n          children: \"Model input\"\n        })]\n      })\n    }), \"\\n\", _jsx(ToolCard, {\n      id: \"vector-visualiser\",\n      title: \"See how data becomes numbers\",\n      description: \"Type in a few short phrases and see how they turn into numeric vectors, then compare how similar they are.\",\n      children: _jsx(VectorVisualiserTool, {})\n    }), \"\\n\", _jsx(QuizBlock, {\n      id: \"ai-foundations-data-quiz\",\n      courseId: \"ai\",\n      levelId: \"foundations\",\n      sectionId: \"ai-foundations-data-and-representation\",\n      title: \"Quick check: data and representation\",\n      questions: [{\n        q: \"In AI, what does data mean\",\n        a: \"Recorded observations about the world that the model learns from.\"\n      }, {\n        q: \"What is the difference between structured and unstructured data\",\n        a: \"Structured fits rows and columns, unstructured is text, images, audio, or logs that need processing.\"\n      }, {\n        q: \"What is a feature\",\n        a: \"A measurable input signal used by the model.\"\n      }, {\n        q: \"What is a label\",\n        a: \"The correct answer used for training.\"\n      }, {\n        q: \"Why can models not understand raw text or images directly\",\n        a: \"They operate on numbers, so inputs must be turned into numeric form.\"\n      }, {\n        q: \"What is a vector in this context\",\n        a: \"A list of numbers that represents an input.\"\n      }, {\n        q: \"What is an embedding for\",\n        a: \"To represent items as numbers so similar items end up near each other.\"\n      }, {\n        q: \"Why do similar things often end up with similar numbers\",\n        a: \"The representation is trained to capture patterns of use and meaning as closeness.\"\n      }, {\n        q: \"Why do we split data into training, validation, and test sets\",\n        a: \"To learn, tune choices, and then do an honest final check without cheating.\"\n      }, {\n        q: \"Name one way bad data creates bad models\",\n        a: \"Wrong labels, missing groups, or outdated data lead to confident but wrong behaviour.\"\n      }]\n    }), \"\\n\", _jsx(SubsectionHeader, {\n      emoji: \"âœ…\",\n      children: _jsx(_components.p, {\n        children: \"After this section you should be able to:\"\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Explain why representation choices change what a model can learn\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Explain what breaks when labels, groups, or context are missing from data\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Explain the trade off between simple features and richer embeddings\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(SectionHeader, {\n      variant: \"content\",\n      emoji: \"ðŸŽ“\",\n      id: \"learning-paradigms\",\n      children: _jsx(_components.p, {\n        children: \"Supervised and unsupervised learning\"\n      })\n    }), \"\\n\", _jsx(SectionProgressToggle, {\n      courseId: \"ai\",\n      levelId: \"foundations\",\n      sectionId: \"ai-foundations-learning-paradigms\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"When we say a model learns, we mean it changes its internal settings so it can make better guesses. It is not learning like a person learns. It is closer to practice. You show examples, it adjusts, and it gets less wrong over time.\"\n    }), \"\\n\", _jsx(GlossaryTip, {\n      term: \"training data\",\n      children: _jsx(_components.p, {\n        children: \"Training data is the set of examples you use to teach the model. It is the practice material, not the final exam.\"\n      })\n    }), \"\\n\", _jsx(GlossaryTip, {\n      term: \"supervised learning\",\n      children: _jsx(_components.p, {\n        children: \"Supervised learning is when a model learns using examples that already have the correct answer attached.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"In supervised learning, you give the model an input and an answer. The model tries to guess the answer, then it is corrected. Over many examples, it learns a pattern that can generalise to new cases.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Email spam filtering is a classic supervised example. You have emails, and you have labels like spam and not spam. Image classification is another. You have images, and you have labels like cat, dog, or receipt. House prices are supervised too, but the answer is a number. The same pattern applies. Inputs in, answer attached, model learns to predict.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"There are two common supervised shapes.\"\n    }), \"\\n\", _jsx(GlossaryTip, {\n      term: \"classification\",\n      children: _jsx(_components.p, {\n        children: \"Classification is predicting a category. For example spam or not spam.\"\n      })\n    }), \"\\n\", _jsx(GlossaryTip, {\n      term: \"regression\",\n      children: _jsx(_components.p, {\n        children: \"Regression is predicting a number. For example a house price.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The difference matters because the mistakes feel different. A wrong category can block a real email. A wrong price can cost real money.\"\n    }), \"\\n\", _jsx(GlossaryTip, {\n      term: \"unsupervised learning\",\n      children: _jsx(_components.p, {\n        children: \"Unsupervised learning is when a model looks for structure in data without being told the correct answers.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Instead of asking \\\"what is the right label\\\", you ask \\\"what patterns exist\\\". This is useful when labels are missing, expensive, or not even well defined.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Grouping customers by behaviour is a common unsupervised example. You might discover that one group buys weekly and another group buys once a year. Topic discovery in documents is another. You might find clusters of themes in support tickets without anyone labelling them by hand. Anomaly detection is a third. You look for unusual behaviour that might signal fraud or intrusion.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Unsupervised learning is harder to evaluate because there is no single correct answer waiting in a spreadsheet. If you change your settings, the groupings can change. Sometimes both results are reasonable. You have to judge usefulness, not just score points.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Here are a few beginner misconceptions to avoid. First, more data is not always better data. If it is biased or messy, you scale the problem. Second, unsupervised learning is not a free shortcut. It still needs careful interpretation. Third, a model learning a pattern does not mean it understands a reason. It means it found a shortcut that worked on the training data.\"\n    }), \"\\n\", _jsx(DiagramBlock, {\n      title: \"Two ways models learn from data\",\n      subtitle: \"Supervised has answers. Unsupervised searches for structure.\",\n      children: _jsxs(\"div\", {\n        className: \"text-xs sm:text-sm leading-5 space-y-2\",\n        children: [_jsx(\"div\", {\n          children: \"Supervised: inputs + labels -> predict a known outcome\"\n        }), _jsx(\"div\", {\n          children: \"Unsupervised: inputs only -> group, compress, or flag unusual patterns\"\n        }), _jsx(\"div\", {\n          children: \"Supervised output: category or number\"\n        }), _jsx(\"div\", {\n          children: \"Unsupervised output: clusters, topics, or anomaly scores\"\n        })]\n      })\n    }), \"\\n\", _jsx(ToolCard, {\n      id: \"tiny-classifier-lab\",\n      title: \"Train a tiny classifier\",\n      description: \"Adjust how much training data you have and how noisy it is. Notice how it changes expected accuracy and stability.\",\n      children: _jsx(TinyClassifierLabTool, {})\n    }), \"\\n\", _jsx(QuizBlock, {\n      id: \"ai-foundations-learning-quiz\",\n      courseId: \"ai\",\n      levelId: \"foundations\",\n      sectionId: \"ai-foundations-learning-paradigms\",\n      title: \"Quick check: supervised and unsupervised learning\",\n      questions: [{\n        q: \"In plain language, what does learning mean for a model\",\n        a: \"It adjusts its internal settings so it makes better guesses from examples.\"\n      }, {\n        q: \"What is training data\",\n        a: \"The examples used to teach the model, not the final test.\"\n      }, {\n        q: \"What makes learning supervised\",\n        a: \"Examples include the correct answer the model should learn to predict.\"\n      }, {\n        q: \"What makes learning unsupervised\",\n        a: \"Examples have no answers, so the model looks for patterns or structure.\"\n      }, {\n        q: \"Give one supervised example\",\n        a: \"Spam filtering, image classification, or house price prediction.\"\n      }, {\n        q: \"Give one unsupervised example\",\n        a: \"Grouping customers, topic discovery, or anomaly detection.\"\n      }, {\n        q: \"What is classification\",\n        a: \"Predicting a category like spam or not spam.\"\n      }, {\n        q: \"What is regression\",\n        a: \"Predicting a number like a house price.\"\n      }, {\n        q: \"Why is unsupervised learning harder to evaluate\",\n        a: \"There is no single correct answer, so usefulness is judged by interpretation and context.\"\n      }, {\n        q: \"Name one common beginner misconception\",\n        a: \"Thinking more data automatically means better results, or that unsupervised learning removes the need for judgement.\"\n      }]\n    }), \"\\n\", _jsx(SubsectionHeader, {\n      emoji: \"âœ…\",\n      children: _jsx(_components.p, {\n        children: \"After this section you should be able to:\"\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Explain when supervised learning is appropriate and what it optimises for\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Explain when unsupervised learning is useful and why evaluation is harder\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Explain what breaks when people treat model outputs as understanding\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(SectionHeader, {\n      variant: \"content\",\n      emoji: \"âš–ï¸\",\n      id: \"responsible-ai\",\n      children: _jsx(_components.p, {\n        children: \"Responsible AI basics and limitations\"\n      })\n    }), \"\\n\", _jsx(SectionProgressToggle, {\n      courseId: \"ai\",\n      levelId: \"foundations\",\n      sectionId: \"ai-foundations-responsible-ai-basics\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"AI systems can cause harm even when everybody is trying to do the right thing. The harm is not always dramatic. Sometimes it is quiet and personal. Someone is incorrectly flagged as suspicious. Someone does not get offered an opportunity. Someone is pushed into a bubble of content that makes them angrier and more certain.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"The core reason is simple. Models learn patterns, not truth. They learn what tends to follow what in the data they were given. They do not know what is fair. They do not know what is lawful. They do not know what is kind. They only know what was rewarded during training.\"\n    }), \"\\n\", _jsx(GlossaryTip, {\n      term: \"bias\",\n      children: _jsx(_components.p, {\n        children: \"Bias is when a system consistently produces unfair or skewed outcomes because of the data or assumptions it was built on.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Bias can enter through the data, through the labels, and through the choices we make. If your training data under represents some faces, facial recognition errors show up first in those groups. If your labels reflect past human decisions, you teach the model those decisions as if they were objective truth. If you optimise only for speed, you often trade away care.\"\n    }), \"\\n\", _jsx(GlossaryTip, {\n      term: \"fairness\",\n      children: _jsx(_components.p, {\n        children: \"Fairness is the practice of reducing avoidable harm across groups and being able to justify outcomes.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Fairness is not a single magic number. It is a set of decisions. What harm do we want to prevent. Which groups matter in this context. What trade offs are acceptable. It belongs with humans, not only with metrics.\"\n    }), \"\\n\", _jsx(GlossaryTip, {\n      term: \"explainability\",\n      children: _jsx(_components.p, {\n        children: \"Explainability is the ability to give a clear, human understandable reason for why a model produced an output.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Explainability matters most when decisions affect real lives. An automated decision system that cannot explain itself becomes hard to challenge. People then learn to distrust the whole process, or worse, they stop trying to challenge it at all.\"\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Bigger models do not automatically mean better or safer. They can be more capable and still be more confusing. They can also be more expensive to run, which encourages shortcuts. If it costs too much to monitor and evaluate, teams quietly stop doing it.\"\n    }), \"\\n\", _jsx(SubsectionHeader, {\n      emoji: \"ðŸŽ¯\",\n      children: _jsx(_components.p, {\n        children: \"What this optimises for\"\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Automation speed and consistency for low risk tasks\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Scaled assistance when humans have time and context\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(SubsectionHeader, {\n      emoji: \"âš ï¸\",\n      children: _jsx(_components.p, {\n        children: \"What this makes harder\"\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Accountability if oversight is vague\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Safe rollback when models drift or fail\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"AI confidence is not the same as correctness. A model can be very confident and still be wrong. This shows up most painfully when people over trust AI answers. A model that sounds fluent can still invent details. It is good at sounding plausible. It is not automatically good at being true.\"\n    }), \"\\n\", _jsx(GlossaryTip, {\n      term: \"hallucination\",\n      children: _jsx(_components.p, {\n        children: \"A hallucination is a confident answer that is not grounded in the input or reality.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"This is why human judgement must stay in control. Use AI to help you think, draft, or explore. Do not let it quietly become the decision maker for high impact outcomes. If a system can deny a benefit, flag a person, or change a life, it needs clear oversight and a way to appeal.\"\n    }), \"\\n\", _jsx(GlossaryTip, {\n      term: \"model drift\",\n      children: _jsx(_components.p, {\n        children: \"Model drift is when real world data changes over time so the model performs worse than it did during evaluation.\"\n      })\n    }), \"\\n\", _jsx(_components.p, {\n      children: \"Drift is not rare. It is normal. People change behaviour. Fraud patterns adapt. Language evolves. Even a simple recommendation system can drift into a bubble because the system is changing the data it then learns from. You cannot evaluate once and declare victory.\"\n    }), \"\\n\", _jsx(DiagramBlock, {\n      title: \"The AI lifecycle and where risks appear\",\n      subtitle: \"Risk shows up at every step. Oversight wraps the loop.\",\n      children: _jsxs(\"div\", {\n        className: \"text-xs sm:text-sm leading-5 space-y-2\",\n        children: [_jsx(\"div\", {\n          children: \"Data collection -> Training -> Evaluation\"\n        }), _jsx(\"div\", {\n          children: \"Deployment -> Monitoring -> Improvement\"\n        }), _jsx(\"div\", {\n          className: \"text-slate-700\",\n          children: \"Human oversight wraps the lifecycle with review, policy, and clear accountability.\"\n        })]\n      })\n    }), \"\\n\", _jsx(ToolCard, {\n      id: \"ai-risk-scenarios\",\n      title: \"Spot AI risks in everyday scenarios\",\n      description: \"Read short AI stories and practice identifying bias, safety risks and over trust in model outputs.\",\n      children: _jsx(ResponsibleAIPlannerTool, {})\n    }), \"\\n\", _jsx(QuizBlock, {\n      id: \"ai-foundations-responsible-quiz\",\n      courseId: \"ai\",\n      levelId: \"foundations\",\n      sectionId: \"ai-foundations-responsible-ai-basics\",\n      title: \"Quick check: responsible AI basics and limitations\",\n      questions: [{\n        q: \"Why can AI cause harm even when well intentioned\",\n        a: \"Models learn patterns from data and can scale mistakes, bias, and bad assumptions.\"\n      }, {\n        q: \"What does it mean that models learn patterns, not truth\",\n        a: \"They learn statistical regularities, not what is correct, fair, or lawful.\"\n      }, {\n        q: \"Name two places bias can enter an AI system\",\n        a: \"Through data coverage, labels, or the assumptions and objectives chosen.\"\n      }, {\n        q: \"Why are facial recognition errors a fairness issue\",\n        a: \"Error rates can be higher for under represented groups, causing unequal harm.\"\n      }, {\n        q: \"Why is bigger not automatically safer\",\n        a: \"More capability can come with more opacity, cost, and operational shortcuts.\"\n      }, {\n        q: \"Why is AI confidence not the same as correctness\",\n        a: \"A model can be confident and still be wrong because confidence is not a truth check.\"\n      }, {\n        q: \"What is a hallucination\",\n        a: \"A confident output that is not grounded in the input or reality.\"\n      }, {\n        q: \"Why is evaluation not a one time step\",\n        a: \"Real world conditions change and models can drift, so performance must be monitored.\"\n      }, {\n        q: \"What is model drift\",\n        a: \"When data or behaviour changes over time so the model performs worse than before.\"\n      }, {\n        q: \"When must humans stay in control\",\n        a: \"When decisions are high impact and affect rights, safety, or access to opportunities.\"\n      }]\n    }), \"\\n\", _jsx(SubsectionHeader, {\n      emoji: \"âœ…\",\n      children: _jsx(_components.p, {\n        children: \"After this section you should be able to:\"\n      })\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"Explain why models do not understand intent or truth and what breaks if you trust them blindly\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Explain why evaluation and monitoring are continuous, not a one time step\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"Explain the trade off between AI automation and human oversight in high impact decisions\"\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(BodyText, {\n      children: _jsx(_components.p, {\n        children: \"You now have the foundations to talk clearly about data, learning, and limits. Intermediate will build on this with evaluation, deployment, and system thinking. Foundations are about how you think, not which tool you use.\"\n      })\n    }), \"\\n\", _jsx(_components.hr, {}), \"\\n\", _jsx(PageNav, {\n      prevHref: \"/ai\",\n      prevLabel: \"AI course overview\",\n      nextHref: \"/ai/intermediate\",\n      nextLabel: \"Intermediate\",\n      showTop: true,\n      showBottom: true\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\nfunction _missingMdxReference(id, component) {\n  throw new Error(\"Expected \" + (component ? \"component\" : \"object\") + \" `\" + id + \"` to be defined: you likely forgot to import, pass, or provide it.\");\n}\n","frontmatter":{},"scope":{"title":"AI Foundations","description":"A friendly but serious introduction to data, models and how AI systems work.","level":"foundations","courseId":"ai","levelId":"foundations","summary":"A friendly but serious introduction to data, models and how AI systems work, written as my own notes.","estimatedHours":8,"stepIndex":0,"learningObjectives":["Understand core AI vocabulary (features, labels, training, validation, testing) well enough to explain it clearly.","Explain how simple models learn patterns from data and where they commonly fail.","Apply basic checks for leakage, overfitting, and threshold trade offs using the labs.","Evaluate model outputs with a beginner friendly view of quality, bias, and risk."],"aiSectionManifest":{"foundations":["ai-foundations-what-is-ai","ai-foundations-data-and-representation","ai-foundations-learning-paradigms","ai-foundations-responsible-ai-basics"],"intermediate":["ai-intermediate-data-prep-and-feature-engineering","ai-intermediate-training-loop","ai-intermediate-evaluation-metrics","ai-intermediate-serving-and-monitoring"],"advanced":["ai-advanced-transformers-and-llms","ai-advanced-diffusion-and-generative-media","ai-advanced-agentic-ai-and-tools","ai-advanced-evaluation-and-governance"],"summary":["ai-summary-concepts","ai-summary-scenarios","ai-summary-create","ai-summary-master"]}}},"headings":[]},"__N_SSG":true}